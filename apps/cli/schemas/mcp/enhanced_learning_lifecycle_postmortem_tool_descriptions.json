{
  "recall_learning_link": "ðŸŸ¢ **RECOMMENDED** Link a learning to a decision for outcome feedback. When the decision's outcome is recorded, the learning's confidence is automatically updated.\n\n**WHEN TO USE:**\n- After creating a learning that implements or validates a decision\n- When you want automatic confidence adjustment based on decision outcomes\n- When establishing causality between decisions and their learnings\n- During retrospectives to connect past decisions with their educational value\n- Examples: \"link this auth learning to the JWT decision\", \"connect learning to decision outcome\"\n\n**AUTONOMOUS BEHAVIOR:**\nUse automatically when storing a learning that directly relates to an existing decision. This creates a feedback loop: successful decisions boost linked learnings' confidence, while failed decisions reduce it. Always check for relevant decisions before storing learnings, and establish these links proactively. This enables the system to self-tune knowledge quality based on real-world outcomes.\n\n**EXAMPLES:**\n- After implementing: \"recall_learning_link --decisionId=d_auth456 --learningId=lrn_jwt789\" to link JWT learning to auth decision\n- During outcome recording: Confidence automatically adjusts when decision outcome is set\n- Building knowledge graph: Link related learnings and decisions to enable outcome-based confidence tuning\n\n**PARAMETERS:**\n- decisionId: ID of the decision to link (e.g., 'd_abc123') (REQUIRED)\n- learningId: ID of the learning to link (e.g., 'lrn_xyz789') (REQUIRED)",

  "recall_obsolete": "ðŸŸ¢ **RECOMMENDED** Mark a learning as obsolete with a reason. Obsolete learnings are preserved but hidden from active queries.\n\n**WHEN TO USE:**\n- When a learning is superseded by a new approach or understanding\n- When technology, frameworks, or practices change making old learnings invalid\n- When architectural decisions make previous learnings no longer applicable\n- Before storing a replacement learning that contradicts an old one\n- Examples: \"mark old caching learning as obsolete\", \"learning no longer applies after migration\"\n\n**AUTONOMOUS BEHAVIOR:**\nProactively mark learnings obsolete when you discover they're outdated or no longer valid. Before storing a learning that contradicts existing knowledge, check for and mark the old learning as obsolete rather than creating conflicting information. This maintains knowledge accuracy and prevents misleading guidance. Always provide a clear reason for obsolescence to create an audit trail.\n\n**EXAMPLES:**\n- Before replacement: \"recall_obsolete --learningId=lrn_old123 --reason='Superseded by new Redis caching approach'\"\n- After migration: \"recall_obsolete --learningId=lrn_mongo456 --reason='No longer applicable after PostgreSQL migration'\"\n- Framework update: \"recall_obsolete --learningId=lrn_react789 --reason='Hooks replaced class components'\"\n\n**PARAMETERS:**\n- learningId: ID of the learning to mark obsolete (REQUIRED)\n- reason: Reason for obsolescence (e.g., 'superseded by new approach', 'no longer applicable') (REQUIRED)",

  "recall_archive": "ðŸŸ¢ **RECOMMENDED** Archive old, low-confidence learnings. Archived learnings are preserved but hidden from active queries.\n\n**WHEN TO USE:**\n- During periodic knowledge base maintenance (monthly/quarterly)\n- When cleaning up stale or unvalidated learnings\n- To improve query performance by reducing noise from inactive knowledge\n- When user asks: \"clean up old learnings\", \"archive stale knowledge\", \"maintenance mode\"\n- Examples: \"archive unused learnings\", \"clean up low-confidence knowledge\"\n\n**AUTONOMOUS BEHAVIOR:**\nUse periodically (suggest every 30-90 days) as part of knowledge base hygiene. Archive learnings that haven't been accessed recently AND have low confidence - these are likely outdated or never validated. This is different from marking obsolete: archiving is automatic cleanup based on metrics, while obsolete is explicit invalidation. Run this proactively during low-activity periods or when storage optimization is needed.\n\n**EXAMPLES:**\n- Quarterly cleanup: \"recall_archive --unusedDays=90 --maxConfidence=0.3\" to archive 90+ day old low-confidence learnings\n- Aggressive cleanup: \"recall_archive --unusedDays=60 --maxConfidence=0.4\" for more aggressive archiving\n- Conservative: \"recall_archive --unusedDays=180 --maxConfidence=0.2\" for gentle archiving\n\n**PARAMETERS:**\n- unusedDays: Archive learnings unused for this many days (default: 90)\n- maxConfidence: Only archive learnings with confidence at or below this threshold (default: 0.3)",

  "recall_learnings_by_status": "ðŸŸ¢ **RECOMMENDED** Retrieve learnings by lifecycle status: 'active', 'obsolete', or 'archived'.\n\n**WHEN TO USE:**\n- To review what learnings are currently active vs obsolete/archived\n- When auditing knowledge base health and lifecycle management\n- To investigate why a learning isn't appearing in normal queries (might be archived)\n- When user asks: \"show obsolete learnings\", \"what's been archived?\", \"review inactive knowledge\"\n- During knowledge base cleanup and validation sessions\n- Examples: \"show archived learnings\", \"list obsolete knowledge\", \"review lifecycle status\"\n\n**AUTONOMOUS BEHAVIOR:**\nUse when investigating knowledge gaps or unexpected query results. If a learning you expect isn't appearing, check archived/obsolete status. During maintenance, review obsolete learnings to verify they're correctly marked, and check archived learnings to potentially restore valuable ones. This tool provides visibility into the full knowledge lifecycle.\n\n**EXAMPLES:**\n- Finding missing knowledge: \"recall_learnings_by_status --status=archived --limit=50\" to review archived items\n- Auditing obsolete: \"recall_learnings_by_status --status=obsolete\" to verify obsolescence reasons\n- Active review: \"recall_learnings_by_status --status=active --limit=100\" to see all active learnings\n\n**PARAMETERS:**\n- status: Lifecycle status: 'active', 'obsolete', or 'archived' (default: 'active')\n- limit: Maximum learnings to return (default: 20)",

  "recall_contradictions": "ðŸŸ¢ **RECOMMENDED** Find records that contradict a given record using AI semantic analysis. Uses embeddings to find similar content, then LLM to detect contradictions.\n\n**WHEN TO USE:**\n- Before storing a new decision to ensure no conflicts with existing ones\n- When reviewing critical decisions to verify consistency\n- During architectural planning to identify conflicting directions\n- When user asks: \"check for contradictions\", \"does this conflict with anything?\", \"verify consistency\"\n- After major architectural changes to find contradictory knowledge\n- Examples: \"check if this decision contradicts anything\", \"find conflicts\"\n\n**AUTONOMOUS BEHAVIOR:**\nALWAYS run this before storing important decisions or architectural learnings. Contradictions create confusion and lead to inconsistent implementations. If contradictions are found, either: 1) Don't store the new record, 2) Mark the old record as obsolete, or 3) Explicitly document why both coexist. Set autoLink=true to automatically create contradiction relationships for visibility. This is critical for maintaining knowledge integrity.\n\n**EXAMPLES:**\n- Pre-storage check: \"recall_contradictions --recordId=d_new456 --minConfidence=0.7 --autoLink=true\" before finalizing decision\n- Consistency audit: \"recall_contradictions --recordId=d_cache789 --minConfidence=0.8\" to find high-confidence conflicts\n- Review mode: \"recall_contradictions --recordId=lrn_api123 --minConfidence=0.6 --autoLink=false\" to just view, not link\n\n**PARAMETERS:**\n- recordId: ID of the record to check for contradictions (e.g., 'i_abc123', 'd_xyz789') (REQUIRED)\n- minConfidence: Minimum confidence threshold 0.0-1.0 for including contradictions (default: 0.7)\n- autoLink: Automatically create 'contradicts' links for high-confidence findings (default: true)",

  "recall_contradiction_check": "ðŸŸ¢ **RECOMMENDED** Check if two specific records contradict each other using AI analysis.\n\n**WHEN TO USE:**\n- When you suspect two specific records might conflict\n- To verify whether an old decision contradicts a new proposal\n- When user asks: \"do these two decisions conflict?\", \"check if X contradicts Y\"\n- Before marking a record obsolete due to another record\n- When investigating why implementations seem inconsistent\n- Examples: \"check if decision X contradicts learning Y\", \"do these conflict?\"\n\n**AUTONOMOUS BEHAVIOR:**\nUse when you've identified two potentially conflicting records and need definitive analysis. This is more targeted than recall_contradictions - use it when you have specific suspects. If contradiction is confirmed, take action: mark one obsolete, create a superseding relationship, or document the coexistence rationale. This validates your instincts about conflicts.\n\n**EXAMPLES:**\n- Pairwise check: \"recall_contradiction_check --record1Id=d_old123 --record2Id=d_new456\" to compare specific decisions\n- Validation: \"recall_contradiction_check --record1Id=lrn_auth789 --record2Id=d_auth456\" to verify learning matches decision\n- Conflict resolution: Use before marking one record obsolete to confirm they actually conflict\n\n**PARAMETERS:**\n- record1Id: ID of the first record to compare (REQUIRED)\n- record2Id: ID of the second record to compare (REQUIRED)",

  "recall_contradiction_summary": "ðŸŸ¢ **RECOMMENDED** Get a summary of all known contradictions in the system.\n\n**WHEN TO USE:**\n- During knowledge base health checks and audits\n- When onboarding to understand areas of uncertainty or conflict\n- Before major architectural decisions to understand existing conflicts\n- When user asks: \"show me all contradictions\", \"what conflicts exist?\", \"knowledge health check\"\n- During retrospectives to identify areas needing clarity\n- Examples: \"show all contradictions\", \"knowledge base conflicts\", \"consistency report\"\n\n**AUTONOMOUS BEHAVIOR:**\nUse periodically (e.g., monthly) as part of knowledge base maintenance. High contradiction counts indicate areas needing attention: either resolve them by marking records obsolete, or document why they coexist. Surface this to users when they're working in areas with many contradictions - they need extra caution. This provides a health metric for knowledge quality.\n\n**EXAMPLES:**\n- Monthly audit: \"recall_contradiction_summary --limit=20\" to review top contradictions\n- Quick check: \"recall_contradiction_summary --limit=5\" for high-priority conflicts\n- Full review: \"recall_contradiction_summary --limit=100\" for comprehensive analysis\n\n**PARAMETERS:**\n- limit: Maximum number of contradiction pairs to show (default: 10)",

  "store_postmortem": "ðŸŸ¡ **IMPORTANT** Create a postmortem record to document a failure, its root cause, lessons learned, and prevention steps.\n\n**WHEN TO USE:**\n- IMMEDIATELY after any significant failure, bug, or incident\n- When a critical issue is resolved and you want to prevent recurrence\n- After production incidents, security breaches, or data loss events\n- When a decision leads to negative outcomes that need documentation\n- During incident retrospectives and post-incident reviews\n- Examples: \"document this auth bypass failure\", \"create postmortem for outage\", \"record this bug's root cause\"\n\n**AUTONOMOUS BEHAVIOR:**\nALWAYS create a postmortem after encountering and fixing significant failures - this is NOT optional. Postmortems are the primary mechanism for learning from mistakes. Include: 1) What happened (facts), 2) Root cause (why), 3) Lessons learned (takeaways), 4) Prevention steps (how to avoid). Link to related decisions if the failure stemmed from a choice. Set appropriate severity based on impact. This creates institutional memory that prevents recurring failures.\n\n**CRITICAL WORKFLOW:**\n1. Immediately after fixing a significant bug/failure\n2. Store the postmortem with all details while fresh\n3. Link to any decisions that contributed (related_decision)\n4. Later, convert lessons to learnings using postmortem_to_learnings\n5. This ensures failures become permanent knowledge\n\n**EXAMPLES:**\n- After incident: \"store_postmortem --title='Auth bypass in JWT validation' --what_happened='Users could access endpoints without valid tokens' --root_cause='Missing null check on decoded token' --lessons_learned=['Always validate token before extracting claims', 'Add integration tests for auth edge cases'] --prevention_steps=['Add token validation wrapper', 'Implement auth testing checklist'] --severity=high --affected_files=['auth/middleware.go', 'auth/jwt.go']\"\n- Production bug: \"store_postmortem --title='Database connection leak' --what_happened='App crashed after 6 hours' --root_cause='Connections not returned to pool' --severity=critical\"\n\n**PARAMETERS:**\n- title: Brief title describing the failure (e.g., 'Authentication bypass in JWT validation') (REQUIRED)\n- what_happened: Detailed description of what went wrong (REQUIRED)\n- root_cause: Analysis of why the failure occurred\n- lessons_learned: Array of key takeaways from this failure\n- prevention_steps: Array of steps to prevent this failure from recurring\n- severity: 'low', 'medium', 'high', or 'critical' (default: medium)\n- affected_files: Array of files affected by this failure\n- related_decision: ID of a decision that led to this failure (e.g., 'd_abc123')\n- related_session: ID of the session where failure occurred",

  "get_postmortems": "ðŸŸ¢ **RECOMMENDED** Retrieve postmortem records with optional filters for status and severity.\n\n**WHEN TO USE:**\n- Before working on similar functionality to check for past failures\n- When user asks: \"what failures have we had?\", \"show me critical postmortems\", \"open incidents\"\n- During sprint planning to identify high-risk areas\n- When investigating patterns of recurring failures\n- To find unresolved postmortems that need attention\n- Examples: \"show critical failures\", \"list open postmortems\", \"what failed in auth?\"\n\n**AUTONOMOUS BEHAVIOR:**\nUse proactively when working in areas with past failures. Before implementing features, check for related postmortems to avoid repeating mistakes. Filter by severity=high or severity=critical to prioritize reviewing serious failures. Filter by status=open to find unresolved issues needing follow-up. This provides historical context and prevents repeated errors.\n\n**EXAMPLES:**\n- Critical failures: \"get_postmortems --severity=critical --limit=10\" to review worst failures\n- Open issues: \"get_postmortems --status=open --limit=20\" to find unresolved postmortems\n- Recent failures: \"get_postmortems --limit=50\" to get comprehensive failure history\n- Recurring problems: \"get_postmortems --status=recurring\" to identify patterns\n\n**PARAMETERS:**\n- status: Filter by status - 'open', 'resolved', or 'recurring'\n- severity: Filter by severity - 'low', 'medium', 'high', or 'critical'\n- limit: Maximum postmortems to return (default: 20)",

  "get_postmortem": "ðŸŸ¢ **RECOMMENDED** Get detailed information about a specific postmortem.\n\n**WHEN TO USE:**\n- When you need full details about a specific failure\n- Before working on files listed in a postmortem's affected_files\n- When investigating why a decision failed (via related_decision)\n- To review lessons learned and prevention steps from a past incident\n- When user asks: \"show me details of postmortem X\", \"what happened in that failure?\"\n- Examples: \"get details on that auth failure\", \"show me postmortem pm_abc123\"\n\n**AUTONOMOUS BEHAVIOR:**\nUse when context_auto_inject or file intel reveals a file has associated postmortems. Always read the full postmortem to understand the failure context, lessons, and prevention steps before making changes to affected files. This prevents reintroducing bugs and ensures you understand the historical context.\n\n**EXAMPLES:**\n- Detailed review: \"get_postmortem --id=pm_abc123\" to see full failure analysis\n- Pre-edit check: After finding postmortem reference in file context, get full details\n- Incident investigation: Get postmortem to understand root cause and lessons\n\n**PARAMETERS:**\n- id: ID of the postmortem (e.g., 'pm_abc123') (REQUIRED)",

  "resolve_postmortem": "ðŸŸ¢ **RECOMMENDED** Mark a postmortem as resolved.\n\n**WHEN TO USE:**\n- After implementing all prevention steps from a postmortem\n- When the root cause has been permanently fixed\n- When user confirms an issue won't recur\n- During postmortem review sessions when cleanup is complete\n- Examples: \"mark postmortem resolved\", \"close this incident\", \"prevention steps implemented\"\n\n**AUTONOMOUS BEHAVIOR:**\nOnly mark postmortems resolved after verifying prevention steps are implemented. Don't auto-resolve - this requires human judgment about whether fixes are sufficient. When implementing prevention steps, create a learning about the implementation and link it to the postmortem, then suggest resolving. Resolved postmortems serve as success stories of learning from failures.\n\n**EXAMPLES:**\n- After prevention: \"resolve_postmortem --id=pm_abc123\" when all prevention steps are complete\n- Incident closed: Mark resolved after confirming the issue is permanently fixed\n- Verification: Only resolve after code review confirms prevention steps are in place\n\n**PARAMETERS:**\n- id: ID of the postmortem to resolve (REQUIRED)",

  "postmortem_stats": "ðŸŸ¢ **RECOMMENDED** Get aggregated statistics about postmortems (counts by status and severity).\n\n**WHEN TO USE:**\n- During team retrospectives and health checks\n- When user asks: \"how many failures have we had?\", \"postmortem summary\", \"incident statistics\"\n- To track progress in resolving postmortems over time\n- When reporting on code quality and reliability metrics\n- To identify if certain severity levels are increasing (red flag)\n- Examples: \"show postmortem stats\", \"how many open failures?\", \"incident metrics\"\n\n**AUTONOMOUS BEHAVIOR:**\nUse to provide context when discussing system reliability. If open or recurring postmortems are high, surface this as a concern. Use these stats to recommend focus areas (e.g., \"You have 5 critical open postmortems - should we address these first?\"). This provides quantitative insight into failure patterns and resolution effectiveness.\n\n**EXAMPLES:**\n- Health check: \"postmortem_stats\" to get overall failure metrics\n- Trending: Run periodically to track if failures are increasing/decreasing\n- Reporting: Use stats in status reports and retrospectives\n\n**PARAMETERS:**\n- None - returns all statistics",

  "postmortem_to_learnings": "ðŸŸ¡ **IMPORTANT** Convert a postmortem's lessons learned into individual learning records. Creates learnings linked to the postmortem.\n\n**WHEN TO USE:**\n- After creating a postmortem with valuable lessons_learned\n- When you want lessons to appear in normal learning queries and context injection\n- To transform incident knowledge into actionable, searchable learnings\n- During postmortem review to formalize lessons into the knowledge base\n- Examples: \"convert postmortem lessons to learnings\", \"formalize these lessons\", \"make lessons searchable\"\n\n**AUTONOMOUS BEHAVIOR:**\nALWAYS call this after storing a postmortem with lessons_learned. Postmortems are isolated incident records; converting lessons to learnings makes them discoverable and actionable. Each lesson becomes a separate learning record with confidence=0.8 (high, since it's based on real failure), linked back to the postmortem. This completes the learning lifecycle: Failure â†’ Postmortem â†’ Lessons â†’ Learnings â†’ Future Prevention.\n\n**CRITICAL WORKFLOW:**\n1. After storing a postmortem: store_postmortem returns an ID\n2. Immediately convert lessons: postmortem_to_learnings with that ID\n3. This makes lessons appear in context_auto_inject for affected files\n4. Future edits to those files will include the failure-derived learnings\n5. This closes the learning loop\n\n**EXAMPLES:**\n- After postmortem: \"postmortem_to_learnings --id=pm_abc123\" to convert all lessons to individual learnings\n- Batch conversion: Run on existing postmortems to populate learning base\n- Completion: Always do this as the final step after creating a postmortem\n\n**PARAMETERS:**\n- id: ID of the postmortem to convert lessons from (REQUIRED)"
}
